{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = 'data1/'\n",
    "\n",
    "# Make a data folder if does not already exist\n",
    "import os\n",
    "if not os.path.exists(path2data):\n",
    "    os.makedirs(path2data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine IDs of articles to scrape\n",
    "ELife since 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database of available articles\n",
    "df = pd.read_csv('PMC-ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keep = df[df['Journal Title']=='eLife']\n",
    "df_keep = df_keep[df_keep['Year']>=2018]\n",
    "df_keep.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_keep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = open('apikey.txt', 'r').read()\n",
    "db = 'pmc'\n",
    "base = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keywords\n",
    "with open('keywords.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    terms = list(reader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of characters to extract around keywords\n",
    "span_buffer = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving parameters\n",
    "N_previous = 0\n",
    "N_chunk = 10000\n",
    "N_save = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load previous computation\n",
    "if N_previous > 0:\n",
    "    dfs_articles = {}\n",
    "    for k in terms:\n",
    "        csv_name = '{:s}{:s}_{:d}.csv'.format(path2data, k, N_previous)\n",
    "        dfs_articles[k] = [pd.read_csv(csv_name, index_col=0)]\n",
    "else:\n",
    "    dfs_articles = defaultdict(list)\n",
    "\n",
    "for i, row in df_keep.loc[N_previous+1:].iterrows():\n",
    "    # Get full text of 1 paper\n",
    "    pmcid = row['PMCID']\n",
    "    s = '{:s}db={:s}&id={:s}'.format(base, db, pmcid, apikey)\n",
    "    out = requests.get(s)\n",
    "    bs = BeautifulSoup(out.content, 'lxml')\n",
    "\n",
    "    # DFs of terms\n",
    "    for term in terms:\n",
    "        dict_term = defaultdict(list)\n",
    "        for s in re.finditer(term, out.text, re.IGNORECASE):\n",
    "            save_span = s.span()\n",
    "            sent = out.text[(save_span[0] - span_buffer):(save_span[1] + span_buffer)]\n",
    "\n",
    "            dict_term['PMCID'].append(pmcid)\n",
    "            dict_term['sentence'].append(sent)\n",
    "        dfs_articles[term].append(pd.DataFrame(dict_term))\n",
    "\n",
    "    # Save output every N\n",
    "    if (i % N_save == 0) & (i > 0):\n",
    "        print(i)\n",
    "        for k in dfs_articles.keys():\n",
    "            df_save = pd.concat(dfs_articles[k])\n",
    "            df_save.to_csv('{}{:s}_{:d}.csv'.format(path2data, k, i))\n",
    "\n",
    "            # Delete last file Unless \n",
    "            if (i-N_save) % N_chunk > 0:\n",
    "                os.remove('{}{:s}_{:d}.csv'.format(path2data, k, i - N_save))\n",
    "\n",
    "        if i % N_chunk == 0:\n",
    "            if i > 0:\n",
    "                dfs_articles = defaultdict(list)\n",
    "            \n",
    "# Save output when finish\n",
    "for k in dfs_articles.keys():\n",
    "    df_save = pd.concat(dfs_articles[k])\n",
    "    df_save.to_csv('{}{:s}_{:d}.csv'.format(path2data, k, i))\n",
    "    os.remove('{}{:s}_{:d}.csv'.format(path2data, k, int(np.round(i-(N_save/2-1), -2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
